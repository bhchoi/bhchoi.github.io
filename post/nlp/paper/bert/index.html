<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>[논문리뷰] Bert: Pre-training of deep bidirectional transformers for language understanding - Even - A super concise theme for Hugo</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="" /><meta name="description" content="Abstract BERT(Bidirectional Encoder Representations from Transformers)는 모든 layer에서 left, right context를 모두 참조하여 unlabeled text로부터 deep bidirectional representations를" /><meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.80.0 with theme even" />


<link rel="canonical" href="http://localhost:1313/post/nlp/paper/bert/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.c7f9d2d0732ffc2e753a30f53d2f9429b5d54824ac9a2962f3803a8b7af7ea71.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="[논문리뷰] Bert: Pre-training of deep bidirectional transformers for language understanding" />
<meta property="og:description" content="Abstract BERT(Bidirectional Encoder Representations from Transformers)는 모든 layer에서 left, right context를 모두 참조하여 unlabeled text로부터 deep bidirectional representations를" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/post/nlp/paper/bert/" />
<meta property="article:published_time" content="2021-03-19T23:25:00+08:00" />
<meta property="article:modified_time" content="2021-03-19T23:25:00+08:00" />
<meta itemprop="name" content="[논문리뷰] Bert: Pre-training of deep bidirectional transformers for language understanding">
<meta itemprop="description" content="Abstract BERT(Bidirectional Encoder Representations from Transformers)는 모든 layer에서 left, right context를 모두 참조하여 unlabeled text로부터 deep bidirectional representations를">
<meta itemprop="datePublished" content="2021-03-19T23:25:00+08:00" />
<meta itemprop="dateModified" content="2021-03-19T23:25:00+08:00" />
<meta itemprop="wordCount" content="2074">



<meta itemprop="keywords" content="bert," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="[논문리뷰] Bert: Pre-training of deep bidirectional transformers for language understanding"/>
<meta name="twitter:description" content="Abstract BERT(Bidirectional Encoder Representations from Transformers)는 모든 layer에서 left, right context를 모두 참조하여 unlabeled text로부터 deep bidirectional representations를"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">킹스날 개발 블로그</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/%20/" class="logo">킹스날 개발 블로그</a>
</div>




<script>
    MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true
        },
        options: {
            skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        }
    };

    window.addEventListener('load', (event) => {
        document.querySelectorAll("mjx-container").forEach(function (x) {
            x.parentElement.classList += 'has-jax'
        })
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
      <a class="menu-item-link" href="/">Home</a>
    </li><li class="menu-item">
      <a class="menu-item-link" href="/post/">Archives</a>
    </li><li class="menu-item">
      <a class="menu-item-link" href="/tags/">Tags</a>
    </li><li class="menu-item">
      <a class="menu-item-link" href="/categories/">Categories</a>
    </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">[논문리뷰] Bert: Pre-training of deep bidirectional transformers for language understanding</h1>

      <div class="post-meta">
        <span class="post-time"> 2021-03-19 </span>
        <div class="post-category">
            <a href="/categories/nlp/"> nlp </a>
            </div>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  <div class="post-toc-content always-active">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#abstract">Abstract</a></li>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#bert">BERT</a>
      <ul>
        <li><a href="#model-architecture">Model Architecture</a></li>
        <li><a href="#inputoutput-representations">Input/Output Representations</a></li>
        <li><a href="#pre-training-bert">Pre-training BERT</a>
          <ul>
            <li><a href="#task-1-masked-lm">Task #1: Masked LM</a></li>
            <li><a href="#task-2-next-sentence-prediction-nsp">Task #2: Next Sentence Prediction (NSP)</a></li>
            <li><a href="#pre-training-data">Pre-training data</a></li>
          </ul>
        </li>
        <li><a href="#fine-tuning-bert">Fine-tuning BERT</a></li>
      </ul>
    </li>
    <li><a href="#ablation-studies">Ablation Studies</a>
      <ul>
        <li><a href="#effect-of-pre-training-tasks">Effect of Pre-training Tasks</a></li>
        <li><a href="#effect-of-model-size">Effect of Model Size</a></li>
        <li><a href="#feature-based-approach-with-bert">Feature-based Approach with BERT</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <h1 id="abstract">Abstract</h1>
<p>BERT(Bidirectional Encoder Representations from Transformers)는 모든 layer에서 left, right context를 모두 참조하여 unlabeled text로부터 deep bidirectional representations를 학습하도록 디자인되었다.</p>
<p>그 결과 pretrained BERT model에 하나의 output layer만 추가하여 fine-tuning을 하여도 다양한 task에서 최고의 성능을 내는 모델을 만들 수 있다.<br>
 </p>
<h1 id="introduction">Introduction</h1>
<p>Language model pretraining은 다양한 NLP task에서 매우 효과적이었다.
Pretrained language repregentation을 down stream에 적용하는 방법은 feature-based와 fine-tuning 2가지가 있다.</p>
<p>ELMo와 같은 feature-based 방식은 pre-trained representations을 추가적인 feature로 사용하는 task-specific architectures를 사용한다.
GPT와 같은 fine-tuning 방식은 pre-trained parameters 전체를 간단하게 미세조정하는 방식이다.</p>
<p>현재의 fine-tuning 방식은 pre-trained representations의 힘을 제한한다고 생각한다. 주요한 한계는 unidirectional 방식으로 학습한다는 것이다. 예를 들어, GPT에서는 left-to-right 아키텍쳐로 인해 self-attention layer에서 모든 토큰이 오직 이전 토큰만 참조 할 수 있다.</p>
<p>본 논문에서는 fine-tuning 방식의 BERT 모델을 제안한다. BERT에서는 masked language model(MLM) objective를 통해 unidirectional 문제를 해결하였다. MLM은 input tokens 중 일부를 랜덤하게 masking하고 이를 예측하는 방식이다. 이를 통해 MLM은 left, right context를 모두 학습할 수 있다. 또한 next sentence prediction(NSP) 방식을 통해 text pair에 대해서도 학습을 한다.<br>
 </p>
<h1 id="bert">BERT</h1>
<p align="center"><img src="/images/nlp/paper/bert/figure_1.png"></p>
<p>BERT는 pre-training과 fine-tuning 두 단계로 학습을 한다. pre-training 단계는 unlabeled data를 이용해 학습을 하고, fine-tuning 단계에서는 downstream task에 대해 pre-trained parameters를 이용하여 labeled data를 미세조정한다.
BERT의 특징은 여러가지 task를 처리할 수 있는 통합 아키텍처라는 것이다.<br>
 </p>
<h2 id="model-architecture">Model Architecture</h2>
<p>BERT의 multi-layer bidirectional transformer encoder로 이루어져 있다.
layers의 수는 L, hiden size는 H, self-attention heads는 A로 표기한다.</p>
<ul>
<li>$BERT_{BASE}$ : L=12, H=768, A=12, Total Parameters=110M</li>
<li>$BERT_{LARGE}$ : L=24, H=1024, A=16, Total Parameters=340M<br>
 </li>
</ul>
<h2 id="inputoutput-representations">Input/Output Representations</h2>
<p>BERT를 이용하여 다양한 task에 적용하기 위해, single sentence와 sentence pair를 모두 표현할 수 있도록 input을 구성하였다. 본 논문에서 말하는 sentence는 연속적인 문장의 임의의 범위이며, 한 문장일 수도 있고 두 문장이 합쳐져 있을 수도 있다.</p>
<p>WordPiece를 이용하여 30,000개의 token을 만들어 사용하였다. 첫번째 token은 항상 special classification token([CLS])이며, classification task에서 aggregate sequence representations로 사용된다.</p>
<p>sentence pairs는 하나의 sequence로 합쳐지며, sentence pairs를 구별하는 2가지 방법이 있다. 첫번째는 special token([SEP])를 사용하는 것이고, 두번째는 모든 토큰에 대해 sentence pair중 어디에 속하는지를 표시하는 embedding을 추가하는 것이다.</p>
<p>위 그림에서 보여지듯이, input embeddings는 E, [CLS]의 final hidden vector는 C, i번째 token의 hidden vector는 T로 표기하였다.</p>
<p>input representation은 token, segment, position embeddings를 summing하여 만들어진다.</p>
<p align="center"><img src="/images/nlp/paper/bert/figure_2.png"></p>
&nbsp;
<h2 id="pre-training-bert">Pre-training BERT</h2>
<p>전통적인 left-to-right, right-to-left model을 사용하지 않고 Masked LM과 Next Sentence Prediction 방법을 이용하여 학습을 한다.<br>
 </p>
<h3 id="task-1-masked-lm">Task #1: Masked LM</h3>
<p>deep bidirectional representation을 학습하기 위해 input token의 일부를 랜덤하게 masking을 하고 이를 예측한다. 이것을 masked LM(MLM)이라고 한다. 모든 실험에서 전체 WordPiece tokens 중 15%를 masking하였다. denoising auto-encoders와 다르게 전체 input을 재구성하는 것이 아니라 masked words를 예측한다.</p>
<p>MLM을 통해 bidirectional pre-trained model을 학습하였으나, pre-training과 fine-tuning 단계 사이의 mismatch가 존재한다. [MASK] token이 fine-tuning 단계에서는 나타나지 않는다는 것이다. 이를 완화시키기위해 15%의 token 중 80%는 masking을 하고, 10%는 랜덤 token으로 교체, 10%는 변경하지 않고 그대로 둔다.<br>
 </p>
<h3 id="task-2-next-sentence-prediction-nsp">Task #2: Next Sentence Prediction (NSP)</h3>
<p>Question Answering, Natural Language Interface와 같이 두 문장 사이의 관계를 학습하는 task를 위해서 next sentence prediction을 수행한다.</p>
<p>sentence A와 B를 선택할때, 50%는 실제로 연결되는 A와 B를 선택하고, 나머지 50%는 랜덤하게 선택한다. 연결되는 경우는 IsNext, 연결되지 않는 경우는 NotNext로 라벨링한다. Figure1에서 볼수있듯이 NSP를 위해 C가 사용된다.<br>
 </p>
<h3 id="pre-training-data">Pre-training data</h3>
<p>BooksCorpus(800M words)와 English Wikipedia(2,500M words)를 사용하였고, Wikipedia에서는 lists, tables, headers를 제외하고 오직 text passages만 사용하였다. long contiguous sequences를 위해서는 document-level의 데이터셋을 사용하는 것이 중요하다.<br>
 </p>
<h2 id="fine-tuning-bert">Fine-tuning BERT</h2>
<p>fine-tuning은 input과 ouput만 적절히 만들어주면 다양한 downstream task에 적용이 가능하다.</p>
<p>output의 token representations는 sequence tagging, question answering과 같은 token level task를 위해 사용되고, [CLS] representation은 entailment, sentiment analysis와 같은 classification을 위해 사용된다.</p>
<p>fine-tuning은 pre-training과 비교해서 상대적으로 적은 비용으로 학습할 수 있다. 본 논문의 모든 결과는 TPU에서는 1시간, GPU에서는 몇 시간이면 결과를 볼 수 있다.<br>
 </p>
<h1 id="ablation-studies">Ablation Studies</h1>
<h2 id="effect-of-pre-training-tasks">Effect of Pre-training Tasks</h2>
<p align="center"><img src="/images/nlp/paper/bert/figure_3.png"></p>
&nbsp;
<h2 id="effect-of-model-size">Effect of Model Size</h2>
<p align="center"><img src="/images/nlp/paper/bert/figure_4.png"></p>
&nbsp;
<h2 id="feature-based-approach-with-bert">Feature-based Approach with BERT</h2>
<p align="center"><img src="/images/nlp/paper/bert/figure_5.png"></p>
    </div>

    
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/bert/">bert</a>
          </div>
      <nav class="post-nav">
        
        <a class="next" href="/post/nlp/paper/attention_is_all_you_need/">
            <span class="next-text nav-default">[논문리뷰] Attention Is All You Need</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:shinejk.qoo@email.com" class="iconfont icon-email" title="email"></a>
      <a href="https://github.com/bhchoi" class="iconfont icon-github" title="github"></a>
  <a href="http://localhost:1313/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2017 - 
    2021<span class="heart"><i class="iconfont icon-heart"></i></span><span></span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.c12618f9a600c40bd024996677e951e64d3487006775aeb22e200c990006c5c7.js"></script>








</body>
</html>
