<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>[논문리뷰] Deep contextualized word representations - Even - A super concise theme for Hugo</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="" /><meta name="description" content="ABSTRACT 본 논문에서는 단어 사용의 문법적, 의미적 복잡성과 언어적 맥락에 따라 어떻게 달라지는지에 대해 모델링하는 deep contextualized word representations를 소개한다" /><meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.80.0 with theme even" />


<link rel="canonical" href="https://bhchoi.github.io/post/nlp/paper/elmo/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.c7f9d2d0732ffc2e753a30f53d2f9429b5d54824ac9a2962f3803a8b7af7ea71.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="[논문리뷰] Deep contextualized word representations" />
<meta property="og:description" content="ABSTRACT 본 논문에서는 단어 사용의 문법적, 의미적 복잡성과 언어적 맥락에 따라 어떻게 달라지는지에 대해 모델링하는 deep contextualized word representations를 소개한다" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bhchoi.github.io/post/nlp/paper/elmo/" />
<meta property="article:published_time" content="2021-04-22T23:25:00+08:00" />
<meta property="article:modified_time" content="2021-04-22T23:25:00+08:00" />
<meta itemprop="name" content="[논문리뷰] Deep contextualized word representations">
<meta itemprop="description" content="ABSTRACT 본 논문에서는 단어 사용의 문법적, 의미적 복잡성과 언어적 맥락에 따라 어떻게 달라지는지에 대해 모델링하는 deep contextualized word representations를 소개한다">
<meta itemprop="datePublished" content="2021-04-22T23:25:00+08:00" />
<meta itemprop="dateModified" content="2021-04-22T23:25:00+08:00" />
<meta itemprop="wordCount" content="1734">



<meta itemprop="keywords" content="elmo,nlp," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="[논문리뷰] Deep contextualized word representations"/>
<meta name="twitter:description" content="ABSTRACT 본 논문에서는 단어 사용의 문법적, 의미적 복잡성과 언어적 맥락에 따라 어떻게 달라지는지에 대해 모델링하는 deep contextualized word representations를 소개한다"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">킹스날 개발 블로그</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">킹스날 개발 블로그</a>
</div>






<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
      <a class="menu-item-link" href="/">Home</a>
    </li><li class="menu-item">
      <a class="menu-item-link" href="/post/">Archives</a>
    </li><li class="menu-item">
      <a class="menu-item-link" href="/tags/">Tags</a>
    </li><li class="menu-item">
      <a class="menu-item-link" href="/categories/">Categories</a>
    </li>
  </ul>
</nav>

<script>
    MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true
        },
        options: {
            skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        }
    };

    window.addEventListener('load', (event) => {
        document.querySelectorAll("mjx-container").forEach(function (x) {
            x.parentElement.classList += 'has-jax'
        })
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">[논문리뷰] Deep contextualized word representations</h1>

      <div class="post-meta">
        <span class="post-time"> 2021-04-22 </span>
        <div class="post-category">
            <a href="/categories/nlp/"> nlp </a>
            </div>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  <div class="post-toc-content always-active">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#abstract">ABSTRACT</a></li>
    <li><a href="#introduction">INTRODUCTION</a></li>
    <li><a href="#elmo-embeddings-from-language-models">ELMo: Embeddings from Language Models</a>
      <ul>
        <li><a href="#bidirectional-language-models">Bidirectional language models</a></li>
        <li><a href="#elmo">ELMo</a></li>
        <li><a href="#using-bilms-for-supervised-nlp-tasks">Using biLMs for supervised NLP tasks</a></li>
        <li><a href="#pre-trained-bidirectional-language-model-architecture">Pre-trained bidirectional language model architecture</a></li>
      </ul>
    </li>
    <li><a href="#evaluation">Evaluation</a></li>
    <li><a href="#analysis">Analysis</a>
      <ul>
        <li><a href="#alternate-layer-weighting-schemes">Alternate layer weighting schemes</a></li>
        <li><a href="#where-to-include-elmo">Where to include ELMo?</a></li>
        <li><a href="#what-information-is-captured-by-the-bilms-representations">What information is captured by the biLM’s representations?</a></li>
        <li><a href="#sample-efficiency">Sample efficiency</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <h1 id="abstract">ABSTRACT</h1>
<p>본 논문에서는 단어 사용의 문법적, 의미적 복잡성과 언어적 맥락에 따라 어떻게 달라지는지에 대해 모델링하는 deep contextualized word representations를 소개한다.<br>
word vector는 대용량의 corpus를 이용한 bidirectional language model(biLM)에 의해 학습된다.<br>
이 word vector는 다른 모델에 쉽게 추가하여 사용할 수 있으며, 6개의 NLP task에서 sota의 성능을 보였다.</p>
<p> </p>
<h1 id="introduction">INTRODUCTION</h1>
<p>많은 neural language understanding model에서 pretraind word representations는 중요한 요소이다. 하지만 단어 사용의 복잡성과 문맥적 특성으로 인해 높은 퀄리티의 representations를 얻는 것은 어려운 일이다. 본 논문에서는 이러한 특성을 고려하고, 기존 모델에 쉽게 적용할 수 있는 deep contextualized word representation을 제안한다.</p>
<p>전통적인 word embeddings와 달리 sentence 전체를 고려하는 representations를 학습한다. 또한 결합된 language model objective를 이용한 bidirectional LSTM에서 생성된 vector를 사용한다. 이러한 이유로 이 모델을 ELMo (Embeddings from Language Models)라고 표현한다.</p>
<p>ELMo는  biML의 모든 layers를 다 사용한다. top layer만 사용하였을때보다 모든 layers를 사용하는 것이 더 좋은 성능을 보인다. 높은 layer는 context-dependent한 부분을 학습하며, 낮은 부분은 syntax적인 부분을 학습하게 된다.</p>
<p> </p>
<h1 id="elmo-embeddings-from-language-models">ELMo: Embeddings from Language Models</h1>
<h2 id="bidirectional-language-models">Bidirectional language models</h2>
<p>N개의 token인 $(t_1, t_2, &hellip;, t_n)$을 가지는 sequence가 주어졌을때, forward language model은 $(t_1, &hellip;, t_{k-1})$를 이용하여 $t_k$의 확률을 모델링 함으로써 sequence의 확률을 계산한다.</p>
<p align="center"><img src="/images/nlp/paper/elmo/eq_1.png"></p>
<p>context-independent token인 $x_k^{LM}$을 L개의 LSTM layer에 통과시킨다. 각각의 k position에서 각 LSTM은 context-dependent representation인 $\vec h_{k,j}^{LM}$을 출력한다. LSTM의 top layer의 output인 $\vec h_{k,L}^{LM}$을 softmax layer에 통과시켜 $t_{k+1}$을 예측하는데 사용한다.<br>
backward language model은 forward language model을 reverse한 것이다.</p>
<p align="center"><img src="/images/nlp/paper/elmo/eq_2.png"></p>
<p>biLM은 forward와 backward를 결합한 것이며 log likelihood를 최대화하도록 학습한다.</p>
<p align="center"><img src="/images/nlp/paper/elmo/eq_3.png"></p>
<p>token representations와 softmax는 forward/backward에서 동일한 parameter를 사용하고, LSTM에서는 각각 사용한다.</p>
<p> </p>
<h2 id="elmo">ELMo</h2>
<p>ELMo는 전체 L layer에 대해 forawrd, backward representations을 결합한 형태로 표현한다.</p>
<p align="center"><img src="/images/nlp/paper/elmo/eq_4.png"></p>
<p>이렇게 계산된 표현을 이용해 각 downstream task에 대해 전체 biLM layer의 가중치를 계산하여 모델링을 하게 된다.</p>
<p align="center"><img src="/images/nlp/paper/elmo/eq_5.png"></p>
<p>$s^{task}$는 softmax-normalized weights로 어떤 layer에 집중해야되는지를 나타내며, $γ^{task}$는 전체 ELMo vector 크기를 결정하는 역할을 한다.</p>
<p> </p>
<h2 id="using-bilms-for-supervised-nlp-tasks">Using biLMs for supervised NLP tasks</h2>
<p>간단한 방법으로 pretrained biLM를 supervised NLP task에 결합하여 성능을 높일 수 있다.
context-independent token $x_k$와 freeze된 ELMo vector를 결합하여, task architecture의 input으로 사용한다.</p>
<p> </p>
<h2 id="pre-trained-bidirectional-language-model-architecture">Pre-trained bidirectional language model architecture</h2>
<p>pre-trained biLM은 양방향 training을 하며, lstm layers 사이에 residual connection을 추가하였다.</p>
<p>최종 모델은 4096 unit을 가지는 bi-LSTM 2개와 512 dimension의 projections, residual connection으로 이루어져 있다. context insensitive type representation은 2048 character의 n-gram convolution filter와 2개의 highway layers를 사용한다.</p>
<p> </p>
<h1 id="evaluation">Evaluation</h1>
<p>ELMo를 추가한 결과 새로운 SOTA를 달성하였다.</p>
<p align="center"><img src="/images/nlp/paper/elmo/table_1.png"></p>
<p> </p>
<h1 id="analysis">Analysis</h1>
<h2 id="alternate-layer-weighting-schemes">Alternate layer weighting schemes</h2>
<p align="center"><img src="/images/nlp/paper/elmo/table_2.png"></p>
regularization parameter λ가 1일 때는 weighting function을 단순히 평균 함수로 만들고, 0.001일 때는 weight를 다양하게 만든다.
<p>weight를 다양하게 만든 것이 평균을 사용하는 것보다 성능이 좋은 것을 확인할 수 있다.</p>
<p> </p>
<h2 id="where-to-include-elmo">Where to include ELMo?</h2>
<p align="center"><img src="/images/nlp/paper/elmo/table_3.png"></p>
기본적으로 input layer에만 ELMo를 추가하였지만, 일부 task에서는 output에도 ELMo를 추가하여 성능을 높일 수 있다는 것을 확인하였다.
<p> </p>
<h2 id="what-information-is-captured-by-the-bilms-representations">What information is captured by the biLM’s representations?</h2>
<p align="center"><img src="/images/nlp/paper/elmo/table_4.png"></p>
<p>glove는 play에 대해 주로 sports에 관련된 것들만 유사어로 추출이 되지만, biLM에서는 sports 뿐만 아니라 연극에 관련된 것도 추출이 되는 것을 볼 수 있다. ELMo는 word vector에서 가지고 있지 않은 문맥 정보를 가지고 있는 것을 알 수 잇다.</p>
<p> </p>
<h2 id="sample-efficiency">Sample efficiency</h2>
<p align="center"><img src="/images/nlp/paper/elmo/figure_1.png"></p>
<p>ELMo를 사용하였을 때 더 적은 training step과 dataset을 사용하여도 비슷한 성능 수준에 도달할 수 있다.</p>

    </div>

    
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/elmo/">elmo</a>
          <a href="/tags/nlp/">nlp</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/nlp/paper/gpt1/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">[논문리뷰] Improving language understanding by generative pre-training</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        <a class="next" href="/post/nlp/paper/albert/">
            <span class="next-text nav-default">[논문리뷰] ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:shinejk.qoo@email.com" class="iconfont icon-email" title="email"></a>
      <a href="https://github.com/bhchoi" class="iconfont icon-github" title="github"></a>
  <a href="https://bhchoi.github.io/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2017 - 
    2021<span class="heart"><i class="iconfont icon-heart"></i></span><span></span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.c12618f9a600c40bd024996677e951e64d3487006775aeb22e200c990006c5c7.js"></script>








</body>
</html>
