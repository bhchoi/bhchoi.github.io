<feed xmlns="http://www.w3.org/2005/Atom"> <id>http://localhost:4000</id><title>개발자</title><subtitle>A minimal, portfolio, sidebar, bootstrap Jekyll theme with responsive web design and focuses on text presentation.</subtitle> <updated>2021-01-18T23:58:48+09:00</updated> <author> <name>choi</name> <uri>http://localhost:4000</uri> </author><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000" rel="alternate" type="text/html" /> <generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator> <rights> © 2021 choi </rights> <icon>/assets/img/favicons/favicon.ico</icon> <logo>/assets/img/favicons/favicon-96x96.png</logo> <entry><title>[논문리뷰]Convolutional neural networks for sentence classification</title><link href="http://localhost:4000/posts/Convolutional_neural_networks_for_sentence_classification/" rel="alternate" type="text/html" title="[논문리뷰]Convolutional neural networks for sentence classification" /><published>2021-01-18T23:11:00+09:00</published> <updated>2021-01-18T23:11:00+09:00</updated> <id>http://localhost:4000/posts/Convolutional_neural_networks_for_sentence_classification/</id> <content src="http://localhost:4000/posts/Convolutional_neural_networks_for_sentence_classification/" /> <author> <name>choi</name> </author> <category term="nlp" /> <summary> https://arxiv.org/abs/1408.5882 Abstract pretraind word vector와 CNN을 sentence classification task에 적용하여 좋은 결과를 얻음 1 Introduction 이번 논문에서는, unsupervised neural language model을 통해 학습한 word vector를 이용하여 cnn을 학습한다. word vector는 google news 천억 단어로 학습되었다. (https://code.google.com/p/word2vec) 먼저 word vector는 static하게 놔두고, 다른 파라미터만 학습을 하였다. 간단한 튜닝을 통해서 많은 benchmark에서 좋은 성능을 얻었다. 2 Model 모델 아키텍... </summary> </entry> <entry><title>BERT를 이용한 한국어 띄어쓰기 모델 만들기 - 04. 학습</title><link href="http://localhost:4000/posts/BERT%EB%A5%BC_%EC%9D%B4%EC%9A%A9%ED%95%9C_%ED%95%9C%EA%B5%AD%EC%96%B4_%EB%9D%84%EC%96%B4%EC%93%B0%EA%B8%B0_04/" rel="alternate" type="text/html" title="BERT를 이용한 한국어 띄어쓰기 모델 만들기 - 04. 학습" /><published>2021-01-14T21:11:00+09:00</published> <updated>2021-01-15T22:05:00+09:00</updated> <id>http://localhost:4000/posts/BERT%EB%A5%BC_%EC%9D%B4%EC%9A%A9%ED%95%9C_%ED%95%9C%EA%B5%AD%EC%96%B4_%EB%9D%84%EC%96%B4%EC%93%B0%EA%B8%B0_04/</id> <content src="http://localhost:4000/posts/BERT%EB%A5%BC_%EC%9D%B4%EC%9A%A9%ED%95%9C_%ED%95%9C%EA%B5%AD%EC%96%B4_%EB%9D%84%EC%96%B4%EC%93%B0%EA%B8%B0_04/" /> <author> <name>choi</name> </author> <category term="nlp" /> <summary> 학습 이제 학습을 수행하는 부분을 작성하겠습니다. 이전에 작성한 config yaml을 불러옵니다. config는 OmegaConf를 이용하여 불러옵니다. from omegaconf import OmegaConf config = OmegaConf.load("config/train_config.yaml")   dataset과 model을 생성합니다. dataset = {} dataset["train"] = CorpusDataset( config.train_data_path, preprocessor.get_input_features ) dataset["val"] = CorpusDataset( config.val_data_path, preprocessor.get_input_featu... </summary> </entry> <entry><title>BERT를 이용한 한국어 띄어쓰기 모델 만들기 - 03. 모델</title><link href="http://localhost:4000/posts/BERT%EB%A5%BC_%EC%9D%B4%EC%9A%A9%ED%95%9C_%ED%95%9C%EA%B5%AD%EC%96%B4_%EB%9D%84%EC%96%B4%EC%93%B0%EA%B8%B0_03/" rel="alternate" type="text/html" title="BERT를 이용한 한국어 띄어쓰기 모델 만들기 - 03. 모델" /><published>2021-01-11T00:11:00+09:00</published> <updated>2021-01-15T22:05:00+09:00</updated> <id>http://localhost:4000/posts/BERT%EB%A5%BC_%EC%9D%B4%EC%9A%A9%ED%95%9C_%ED%95%9C%EA%B5%AD%EC%96%B4_%EB%9D%84%EC%96%B4%EC%93%B0%EA%B8%B0_03/</id> <content src="http://localhost:4000/posts/BERT%EB%A5%BC_%EC%9D%B4%EC%9A%A9%ED%95%9C_%ED%95%9C%EA%B5%AD%EC%96%B4_%EB%9D%84%EC%96%B4%EC%93%B0%EA%B8%B0_03/" /> <author> <name>choi</name> </author> <category term="nlp" /> <summary> Config 설정 학습에 필요한 각종 값과 하이퍼파라미터입니다. argparse를 이용해 인자로 받을 수도 있으나, yaml로 정의하여 불러오도록 하겠습니다. task: korean_spacing_20210101 log_path: logs bert_model: monologg/kobert train_data_path: data/nikl_newspaper/train.txt val_data_path: data/nikl_newspaper/val.txt test_data_path: data/nikl_newspaper/test.txt max_len: 128 train_batch_size: 128 eval_batch_size: 128 dropout_rate: 0.1 gpus: 8 distributed_back... </summary> </entry> <entry><title>BERT를 이용한 한국어 띄어쓰기 모델 만들기 - 02. 데이터 전처리</title><link href="http://localhost:4000/posts/BERT%EB%A5%BC_%EC%9D%B4%EC%9A%A9%ED%95%9C_%ED%95%9C%EA%B5%AD%EC%96%B4_%EB%9D%84%EC%96%B4%EC%93%B0%EA%B8%B0_02/" rel="alternate" type="text/html" title="BERT를 이용한 한국어 띄어쓰기 모델 만들기 - 02. 데이터 전처리" /><published>2021-01-10T00:11:00+09:00</published> <updated>2021-01-15T22:04:06+09:00</updated> <id>http://localhost:4000/posts/BERT%EB%A5%BC_%EC%9D%B4%EC%9A%A9%ED%95%9C_%ED%95%9C%EA%B5%AD%EC%96%B4_%EB%9D%84%EC%96%B4%EC%93%B0%EA%B8%B0_02/</id> <content src="http://localhost:4000/posts/BERT%EB%A5%BC_%EC%9D%B4%EC%9A%A9%ED%95%9C_%ED%95%9C%EA%B5%AD%EC%96%B4_%EB%9D%84%EC%96%B4%EC%93%B0%EA%B8%B0_02/" /> <author> <name>choi</name> </author> <category term="nlp" /> <summary> 띄어쓰기 태깅 먼저 띄어쓰기에 대한 태깅 합니다. NER과 동일한 방법으로 각 토큰을 BI로 표현해보겠습니다. word 기준이 아닌 char 기준으로 태깅을 합니다. sentence = "그 외 기간은 관계자 외 출입금지입니다.".split(" ") tags = [] for word in sentence: for i in range(len(word)): if i == 0: tags.append("B") else: tags.append("I") &amp;gt;&amp;gt; tags ['B', 'B', 'B', 'I', 'I', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I']... </summary> </entry> <entry><title>BERT를 이용한 한국어 띄어쓰기 모델 만들기 - 01. 데이터 준비</title><link href="http://localhost:4000/posts/BERT%EB%A5%BC_%EC%9D%B4%EC%9A%A9%ED%95%9C_%ED%95%9C%EA%B5%AD%EC%96%B4_%EB%9D%84%EC%96%B4%EC%93%B0%EA%B8%B0_01/" rel="alternate" type="text/html" title="BERT를 이용한 한국어 띄어쓰기 모델 만들기 - 01. 데이터 준비" /><published>2021-01-08T00:11:00+09:00</published> <updated>2021-01-15T22:04:06+09:00</updated> <id>http://localhost:4000/posts/BERT%EB%A5%BC_%EC%9D%B4%EC%9A%A9%ED%95%9C_%ED%95%9C%EA%B5%AD%EC%96%B4_%EB%9D%84%EC%96%B4%EC%93%B0%EA%B8%B0_01/</id> <content src="http://localhost:4000/posts/BERT%EB%A5%BC_%EC%9D%B4%EC%9A%A9%ED%95%9C_%ED%95%9C%EA%B5%AD%EC%96%B4_%EB%9D%84%EC%96%B4%EC%93%B0%EA%B8%B0_01/" /> <author> <name>choi</name> </author> <category term="nlp" /> <summary> Pretrained BERT, PYTORCH를 이용해 한국어 띄어쓰기 모델을 만들어 보겠습니다. 한국어 데이터셋 한국어 띄어쓰기 모델을 학습하기 위해서는 띄어쓰기가 잘 되어 있는 데이터셋이 필요합니다. 세종 코퍼스, 모두의 말뭉치 처럼 오픈 데이터셋도 있고, 네이버 뉴스 같은 것을 직접 크롤링 할 수도 있습니다. 이번에는 최근에 공개된 모두의 말뭉치 중에서 신문 말뭉치를 이용해보겠습니다. 모두의 말뭉치는 말뭉치 신청 후 승인이 되면 다운로드가 가능합니다. 데이터셋 전처리 말뭉치를 다운로드 받으면 제목, 기자, 날짜, 토픽, 내용 등이 json 형태로 되어 있어 파싱 후 사용해야 됩니다. 직접 파싱을 할 수도 있으나, 감사하게도 Korpora라는 라이브러리를 만들어 주셔서 편하게 데이터셋... </summary> </entry> </feed>
